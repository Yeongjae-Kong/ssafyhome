import os
import re
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate

try:
    from langchain_classic.chains import create_retrieval_chain
    from langchain_classic.chains.combine_documents import create_stuff_documents_chain
except ImportError:
    from langchain.chains import create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain

load_dotenv()

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

vector_store = None
retriever = None 
llm = None

SSAFY_GMS_BASE_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1"

@app.on_event("startup")
async def startup_event():
    global vector_store, retriever, llm
    
    pdf_files = ["report.pdf", "KBì£¼íƒì‹œì¥ë¦¬ë·°_2025ë…„ 12ì›”í˜¸.pdf", "GTX.pdf", "2026ë…„ í•œêµ­ ê²½ì œ ë° ë¶€ë™ì‚° ì‹œì¥ ì „ë§ í†µí•© ë³´ê³ ì„œ.pdf"]
    all_splits = [] 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

    print("ğŸ“„ PDF íŒŒì¼ ë¡œë”© ë° í†µí•© ì‹œì‘...")

    for pdf_path in pdf_files:
        if os.path.exists(pdf_path):
            print(f"   Reading: {pdf_path}...")
            try:
                loader = PyPDFLoader(pdf_path)
                documents = loader.load()
                splits = text_splitter.split_documents(documents)
                all_splits.extend(splits)
                print(f"   âœ… {pdf_path} ë¡œë“œ ì™„ë£Œ ({len(splits)} chunks)")
            except Exception as e:
                print(f"   âŒ {pdf_path} ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        else:
            print(f"   âš ï¸ íŒŒì¼ ì—†ìŒ: {pdf_path} (ê±´ë„ˆëœ€)")

    if not all_splits:
        print("âŒ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“Š ì´ {len(all_splits)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤...")

    embedding_model = OpenAIEmbeddings(
        model="text-embedding-3-small", 
        base_url=SSAFY_GMS_BASE_URL,
        chunk_size=10 
    )

    vector_store = Chroma.from_documents(
        documents=all_splits, 
        embedding=embedding_model
    )
    
    llm = ChatOpenAI(
        model_name="gpt-4o", 
        temperature=0.3, 
        base_url=SSAFY_GMS_BASE_URL
    )
    
    # ê²€ìƒ‰ ë²”ìœ„ë¥¼ ë„‰ë„‰í•˜ê²Œ ì¡ì•„ì„œ ë¬¸ë§¥ ë¶€ì¡± í˜„ìƒ ì™„í™”
    retriever = vector_store.as_retriever(search_kwargs={"k": 6})
    print("âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! (SSAFY GMS Connected)")


class AnalyzeRequest(BaseModel):
    region: str
    query: str = ""
    analysis_type: str = "detailed" 

@app.post("/api/rag/analyze")
async def analyze_real_estate(req: AnalyzeRequest):
    if not retriever or not llm:
        raise HTTPException(status_code=500, detail="RAG ì„œë²„ ì´ˆê¸°í™” ì‹¤íŒ¨")
    
    # ------------------------------------------------------------------
    # [1] ì ìˆ˜ ê°€ì´ë“œë¼ì¸
    # ------------------------------------------------------------------
    score_rules = (
        "### ğŸš¨ ì ìˆ˜ ì±…ì • ê·œì¹™ (1~7ì  ì²™ë„):"
        "ë‹¹ì‹ ì€ ì ê·¹ì ì¸ íˆ¬ì ì „ëµê°€ì…ë‹ˆë‹¤. 'ë³´ë¥˜'ë‚˜ 'íŒë‹¨ ë¶ˆê°€'ëŠ” ìµœëŒ€í•œ í”¼í•˜ì„¸ìš”."
        "ì‘ì€ íŒíŠ¸ë¼ë„ ì°¾ì•„ì„œ ë°˜ë“œì‹œ ë§¤ìˆ˜/ë§¤ë„ ë°©í–¥ì„±ì„ ì œì‹œí•˜ì„¸ìš”."
        "\n"
        "- **[SCORE:1]** (ì ê·¹ ë§¤ë„ ì¶”ì²œ): ì‹œì¥ ë¶•ê´´, ì‹¬ê°í•œ ì•…ì¬."
        "- **[SCORE:2]** (ë§¤ë„ ì¶”ì²œ): í•˜ë½ ì¶”ì„¸."
        "- **[SCORE:3]** (ë¹„ì¤‘ ì¶•ì†Œ ì¶”ì²œ): í˜¸ì¬ë³´ë‹¤ ì•…ì¬ ìš°ìœ„."
        "- **[SCORE:4]** (ê´€ë§): (ê°€ê¸‰ì  ì‚¬ìš© ê¸ˆì§€) ë°©í–¥ì„± ë¶ˆë¶„ëª…."
        "- **[SCORE:5]** (ì†Œê·¹ì  ë§¤ìˆ˜ ì¶”ì²œ): ë°”ë‹¥ ë‹¤ì§€ê¸°, ê¸ì • ì‹ í˜¸."
        "- **[SCORE:6]** (ë§¤ìˆ˜ ì¶”ì²œ): ìƒìŠ¹ ì¶”ì„¸, í˜¸ì¬ ëª…í™•."
        "- **[SCORE:7]** (ì ê·¹ì  ë§¤ìˆ˜ ì¶”ì²œ): ì €í‰ê°€ + ëŒ€í˜• í˜¸ì¬."
        "\n"
        "**ë‹µë³€ì˜ ë§¨ ì²« ì¤„ì€ ë°˜ë“œì‹œ [SCORE:ì ìˆ˜] íƒœê·¸ë¡œ ì‹œì‘í•˜ì„¸ìš”.**"
    )

    # ------------------------------------------------------------------
    # [2] ë¶„ì„ ë° ì‘ì„± ê°€ì´ë“œ
    # ------------------------------------------------------------------
    analysis_rules = (
        "### ğŸ“ ë¶„ì„ ë° ì‘ì„± ì›ì¹™ (í•„ìˆ˜ ì¤€ìˆ˜):"
        "1. **ë¬¸ì„œ ì¸ìš©:** [Context]ì— í•´ë‹¹ ì§€ì—­ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì ê·¹ ì¸ìš©í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."
        "2. **ì ê·¹ì  ì¶”ë¡ :** ë¬¸ì„œì— íŠ¹ì • ë™/êµ¬ ë‹¨ìœ„ ì •ë³´ê°€ ì—†ë”ë¼ë„ ì ˆëŒ€ë¡œ 'ì •ë³´ê°€ ì—†ë‹¤'ê³  ëë‚´ì§€ ë§ˆì„¸ìš”."
        "   - [Context]ì˜ **'ì„œìš¸/ìˆ˜ë„ê¶Œ ì „ì²´ íë¦„'**, **'ê±°ì‹œ ê²½ì œ'** ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•˜ì„¸ìš”."
        "   - ì¼ë°˜ì ì¸ ë¶€ë™ì‚° ì§€ì‹(ì…ì§€, í•™êµ°, êµí†µ)ì„ ê²°í•©í•˜ì—¬ ë¶„ì„ì„ ì™„ì„±í•˜ì„¸ìš”."
        "3. **ì„œìˆ  ë°©ì‹:**"
        "   - ê°€ì¥ ì²« ì¤„ [SCORE] íƒœê·¸ë¥¼ ì œì™¸í•˜ê³ , **ëª¨ë“  ë³¸ë¬¸ì€ ìì—°ìŠ¤ëŸ¬ìš´ ì¤„ê¸€(Prose)** í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”."
        "   - **'#', '**', '-', '1.' ë“±ì˜ ë§ˆí¬ë‹¤ìš´/íŠ¹ìˆ˜ë¬¸ì ì‚¬ìš©ì„ ê¸ˆì§€í•©ë‹ˆë‹¤.**"
        "   - ì „ë¬¸ê°€ê°€ ì˜†ì—ì„œ ë§í•´ì£¼ëŠ” ê²ƒì²˜ëŸ¼ í¸ì•ˆí•˜ê³  ì „ë¬¸ì ì¸ ë¬¸ì²´ë¡œ ì‘ì„±í•˜ì„¸ìš”."
    )

    # ------------------------------------------------------------------
    # [3] í”„ë¡¬í”„íŠ¸ ë¶„ê¸° (ìš”ì•½ vs ìƒì„¸)
    # ------------------------------------------------------------------
    if req.analysis_type == "summary":
        system_instructions = (
            f"ë‹¹ì‹ ì€ ë¶€ë™ì‚° ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. Contextë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.\n"
            f"{score_rules}\n"
            f"{analysis_rules}\n"
            "**ë¶„ëŸ‰:** ì „ì²´ ë‚´ìš©ì„ **5~6ì¤„ ë‚´ì™¸**ì˜ ë‘ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
        )
    else:
        system_instructions = (
            f"ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ë¶€ë™ì‚° ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. Contextë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µ ë¶„ì„í•˜ì„¸ìš”.\n"
            f"{score_rules}\n"
            f"{analysis_rules}\n"
            "**ë¶„ëŸ‰ ë° êµ¬ì„±:** ì‹œì¥ í˜„í™©, ì…ì§€ ë¶„ì„, ë¦¬ìŠ¤í¬, íˆ¬ì ì „ëµ ìˆœìœ¼ë¡œ íë¦„ì„ ì¡ì•„ **ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ(6~7 ë¬¸ë‹¨), ì§€ì—­ì˜ ì…ì§€ë¥¼ ì¤‘ì ìœ¼ë¡œ** ì„œìˆ í•˜ì„¸ìš”."
        )

    # [âœ… í•µì‹¬ ìˆ˜ì •] ì „ì²´ë¥¼ f-string í•˜ë‚˜ë¡œ ë¬¶ê³  contextë§Œ ì´ì¤‘ ì¤‘ê´„í˜¸ ì²˜ë¦¬
    # ì´ë ‡ê²Œ í•˜ë©´ Pythonì´ {{context}}ë¥¼ {context}ë¡œ ë³€í™˜í•´ì£¼ê³ , 
    # LangChainì€ {context}ë¥¼ ë³´ê³  ë³€ìˆ˜ ìœ„ì¹˜ë¥¼ ì¸ì‹í•©ë‹ˆë‹¤.
    system_prompt = f"{system_instructions}\n\n[Context]:\n{{context}}"

    prompt_template = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])
    
    # ë¬¸ì„œ ê²°í•© ì²´ì¸
    chain = create_stuff_documents_chain(llm, prompt_template)
    rag_chain = create_retrieval_chain(retriever, chain)
    
    user_input = f"{req.region} ì§€ì—­ì˜ 2025ë…„ ë¶€ë™ì‚° ì‹œì¥ ì „ë§. {req.query}"
    
    try:
        response = rag_chain.invoke({"input": user_input})
        raw_answer = response["answer"]
        
        # ì ìˆ˜ íŒŒì‹±
        score_match = re.search(r'\[SCORE:(\d)\]', raw_answer)
        score = 4 
        clean_answer = raw_answer

        if score_match:
            score = int(score_match.group(1))
            clean_answer = raw_answer.replace(score_match.group(0), "").strip()

        return {
            "score": score,
            "result": clean_answer
        }
        
    except Exception as e:
        print(f"Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)